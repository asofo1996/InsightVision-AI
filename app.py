import streamlit as st
import os, tempfile, subprocess, torch, cv2, re
from PIL import Image
import whisper
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# âœ… í™˜ê²½ ë³€ìˆ˜ ë° Supabase
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# âœ… íŒŒì¼ëª… ë¶„í•´
def parse_title_kor(filename):
    filename = os.path.splitext(filename)[0].replace("(", "_").replace(")", "_")
    parts = re.split(r"[_\\-]+", filename)
    client = parts[0] if len(parts) > 0 else "ë¯¸ì§€ì •"
    category = parts[1] if len(parts) > 1 else "ê¸°íƒ€"
    subcontext = "_".join(parts[2:]) if len(parts) > 2 else ""
    return {"client": client, "category": category, "subcontext": subcontext}

# âœ… DB ë¡œë”©
def fetch_previous_summaries_by_category(category):
    try:
        result = supabase.table("analysis_results").select("summary_text").eq("category", category).order("created_at", desc=True).limit(5).execute()
        return [r["summary_text"] for r in result.data if r["summary_text"]]
    except: return []

def fetch_experiences_by_category(category):
    try:
        result = supabase.table("performance_logs").select("experience, ctr").neq("experience", "").order("recorded_at", desc=True).limit(10).execute()
        return [f"- {r['experience']} (CTR: {r['ctr']}%)" for r in result.data if r["experience"]]
    except: return []

# âœ… í…ìŠ¤íŠ¸ ìš”ì•½
def summarize_all_inputs(frames_desc, transcript, title, prompt):
    return f"""ğŸ¬ ê´‘ê³  ì½˜í…ì¸  ì •ë°€ ë¶„ì„

ğŸ“Œ ê´‘ê³  ì œëª©: {title}

ğŸ–¼ï¸ ì‹œê° ì½˜í…ì¸  ìš”ì•½:
{chr(10).join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)])}

ğŸ™ï¸ í…ìŠ¤íŠ¸ ìš”ì•½:
{transcript}

ğŸ” ë¶„ì„ ëª©ì :
- ì—…ì¢…/ì¢…ëª©ê³¼ ì „ëµì  ì í•©ì„± í‰ê°€
- êµ­ë‚´ íƒ€ê²Ÿê³¼ì˜ ì •í•©ì„±
- ì „í™˜ìœ¨ ê´€ì ì—ì„œì˜ ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì‹¤ë¬´ ê°œì„  ì „ëµ ë„ì¶œ (3ê°€ì§€ ì´ìƒ)

ğŸ’¡ ì‚¬ìš©ì ìš”ì²­:
{prompt.strip()}
"""

# âœ… Ollamaìš© ì‹¬í™” ë¶„ì„ í”„ë¡¬í”„íŠ¸
def generate_ollama_prompt(prompt_text, category, file_name, descriptions, transcript, client, subcontext):
    context_intro = "\n".join(fetch_previous_summaries_by_category(category))
    exp_intro = "\n".join(fetch_experiences_by_category(category))
    return f"""
[ê´‘ê³  ë¶„ì„ ëŒ€ìƒ]
- ê³ ê°ì‚¬: {client}
- ì—…ì¢…/ë¶„ì•¼: {category}
- ì„¸ë¶€ ë¬¸ë§¥: {subcontext}
- íŒŒì¼ëª…: {file_name}

[ì‹œê° ì„¤ëª… ìš”ì•½]
{chr(10).join(descriptions)}

[í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ìš”ì•½]
{transcript}

[ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ìš”ì•½]
{context_intro}

[ì„±ê³¼ ê¸°ë°˜ ê´‘ê³  ê²½í—˜]
{exp_intro}

[ì •ë°€ ë¶„ì„ ìš”ì²­ í•­ëª©]
1. ì—…ì¢… ë° íƒ€ê²Ÿê³¼ì˜ ì „ëµì  ì •í•©ì„± ë¶„ì„ (Hook êµ¬ì¡°, ì½˜í…ì¸  í†¤, CTA ë“±)
2. ì‹œì²­ì ìœ ì§€ë ¥ê³¼ í´ë¦­ ìœ ë„ë ¥ í‰ê°€
3. CTRê³¼ ì „í™˜ìœ¨ ê´€ì ì—ì„œ ì„±ê³¼ê°€ ë†’ì€ ì½˜í…ì¸ ì™€ ë¹„êµ
4. ì•/ì¤‘/ë ì½˜í…ì¸  êµ¬ì¡°ì˜ ê°œì„  ë°©ì•ˆ
5. ìµœì†Œ 3ê°€ì§€ ì´ìƒì˜ ì‹¤ë¬´ ì¤‘ì‹¬ ê°œì„  ì „ëµ ì œì‹œ
"""

# âœ… Ollama ì‹¤í–‰
def analyze_with_ollama(prompt_text):
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=PromptTemplate.from_template("{prompt_text}"), llm=llm)
    return chain.run(prompt_text=prompt_text)

# âœ… DB ì €ì¥
def save_analysis_to_db(client, file, category, subcontext, summary, transcript, descriptions, prompt, ctype):
    supabase.table("analysis_results").insert({
        "client_name": client,
        "category": category,
        "subcontext": subcontext,
        "content_type": ctype,
        "file_name": file,
        "summary_text": summary,
        "raw_transcript": transcript,
        "frame_descriptions": descriptions,
        "prompt_used": prompt,
        "created_at": datetime.utcnow().isoformat()
    }).execute()

def save_performance_to_db(client, file, views, clicks, conversion, ctr, experience):
    supabase.table("performance_logs").insert({
        "client_name": client,
        "file_name": file,
        "views": views,
        "clicks": clicks,
        "conversion": conversion,
        "ctr": ctr,
        "experience": experience,
        "recorded_at": datetime.utcnow().isoformat()
    }).execute()

# âœ… ëª¨ë¸ ìºì‹±
@st.cache_resource
def load_blip():
    return (
        BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base"),
        BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    )

def describe_image_with_blip(image):
    processor, model = load_blip()
    inputs = processor(image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def transcribe_audio_whisper(audio_path):
    model = whisper.load_model("base")
    return model.transcribe(audio_path, fp16=torch.cuda.is_available())['text']

def extract_keyframes(video_path, interval_sec=1):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(fps * interval_sec)
    frames, count = [], 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        if count % interval == 0:
            path = os.path.join(tempfile.gettempdir(), f"frame_{count}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
        count += 1
    cap.release()
    return frames

# âœ… UI ì‹œì‘
st.set_page_config(page_title="AI ì½˜í…ì¸  ì „ëµ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“Š ì‹œì˜¨ë§ˆì¼€íŒ… ì½˜í…ì¸  ì „ëµ ë¶„ì„ ì‹œìŠ¤í…œ")

prompt_text = st.text_area(
    "âœï¸ ë¶„ì„ ìš”ì²­ ë©”ì‹œì§€",
    "ê´‘ê³  ì½˜í…ì¸ ì˜ íƒ€ê²Ÿ, ì „ëµ, ë©”ì‹œì§€, êµ¬ì„± ì¸¡ë©´ì—ì„œ ì •ë°€ ë¶„ì„í•˜ê³  ê°œì„  ì „ëµì„ 3ê°€ì§€ ì´ìƒ ì œì•ˆí•´ ì£¼ì„¸ìš”."
)

# âœ… ì´ë¯¸ì§€ ë¶„ì„
uploaded_image = st.file_uploader("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
if uploaded_image:
    img = Image.open(uploaded_image).convert("RGB")
    st.image(img, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_container_width=True)

    if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘..."):
            image_desc = describe_image_with_blip(img)

        parsed = parse_title_kor(uploaded_image.name)
        full_prompt = f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„¤ëª…: {image_desc}\n\n{prompt_text}"
        with st.spinner("ì „ëµ ë¶„ì„ ì¤‘..."):
            result = analyze_with_ollama(full_prompt)

        save_analysis_to_db(parsed["client"], uploaded_image.name, parsed["category"], parsed["subcontext"], result, "", [image_desc], prompt_text, "image")
        st.success("âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ")
        st.subheader("ğŸ§  ë¶„ì„ ê²°ê³¼")
        st.markdown(result)

# âœ… ì˜ìƒ ë¶„ì„
uploaded_video = st.file_uploader("ğŸ¥ ì˜ìƒ ì—…ë¡œë“œ", type=["mp4", "mov"])
if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
    st.video(video_path)

    if st.button("ğŸ” 1ì°¨ ë¶„ì„ ì‹¤í–‰"):
        with st.spinner("í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."):
            frames = extract_keyframes(video_path)
            descs = [describe_image_with_blip(Image.open(f)) for f in frames]

        with st.spinner("ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘..."):
            audio_path = os.path.join(tempfile.gettempdir(), "audio.wav")
            subprocess.run(["ffmpeg", "-y", "-i", video_path, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path], stdout=subprocess.DEVNULL)
            transcript = transcribe_audio_whisper(audio_path)

        parsed = parse_title_kor(uploaded_video.name)
        summary = summarize_all_inputs(descs, transcript, uploaded_video.name, prompt_text)
        result = analyze_with_ollama(summary)

        st.success("âœ… 1ì°¨ ë¶„ì„ ì™„ë£Œ")
        st.subheader("ğŸ§  ì „ëµ ë¶„ì„ ê²°ê³¼")
        st.markdown(result)

        # âœ… 2ì°¨ ì •ë°€ ë¶„ì„ ë²„íŠ¼
        if st.button("ğŸ“Œ ë” ì •ë°€í•œ ì‹¤ì „ ì „ëµ ì†”ë£¨ì…˜ ìš”ì²­"):
            with st.spinner("ğŸ§  ê³ ë„í™”ëœ ì „ëµ ë¶„ì„ ì¤‘..."):
                full_prompt = generate_ollama_prompt(prompt_text, parsed["category"], uploaded_video.name, descs, transcript, parsed["client"], parsed["subcontext"])
                deep_result = analyze_with_ollama(full_prompt)

            if deep_result:
                st.success("âœ… ì •ë°€ ì „ëµ ë¶„ì„ ì™„ë£Œ")
                st.subheader("ğŸ’¡ ê³ ë„í™”ëœ ì‹¤ì „ ì „ëµ ì œì•ˆ")
                st.markdown(deep_result)
                save_analysis_to_db(parsed["client"], uploaded_video.name, parsed["category"], parsed["subcontext"], deep_result, transcript, descs, prompt_text, "video")
            else:
                st.error("âŒ ì •ë°€ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ë˜ëŠ” Ollama ì„¤ì •ì„ ì ê²€í•´ ì£¼ì„¸ìš”.")

# âœ… ê´‘ê³  ì„±ê³¼ ì…ë ¥
st.markdown("---")
st.header("ğŸ“Š ê´‘ê³  ì„±ê³¼ ì…ë ¥")
with st.form("performance_form"):
    file_name = st.text_input("íŒŒì¼ëª… (ì˜ˆ: ë³‘ì›ëª…_ë¶„ì•¼_ë‚ ì§œ_ë²„ì „.mp4)", "")
    parsed = parse_title_kor(file_name)
    views = st.number_input("ì¡°íšŒìˆ˜", min_value=0)
    clicks = st.number_input("í´ë¦­ìˆ˜", min_value=0)
    conversion = st.number_input("ì „í™˜ìˆ˜", min_value=0)
    ctr = round((clicks / views) * 100, 2) if views else 0.0
    experience = st.text_area("ğŸ“ ê´‘ê³  ê²½í—˜ ë©”ëª¨", placeholder="ì˜ˆ: í•œì§€ ë°°ê²½ ë„£ì—ˆë”ë‹ˆ CTR ìƒìŠ¹")
    submitted = st.form_submit_button("ì„±ê³¼ + ê²½í—˜ ì €ì¥")
    if submitted and file_name:
        save_performance_to_db(parsed["client"], file_name, views, clicks, conversion, ctr, experience)
        st.success(f"âœ… {parsed['client']} ê´‘ê³  ê²½í—˜ ì €ì¥ ì™„ë£Œ")

st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
