import streamlit as st
import os
import shutil
import glob
import cv2
import tempfile
import subprocess
import torch
import pytesseract
import whisper
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

st.set_page_config(page_title="AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ¬ AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")
prompt_text = st.text_area("ë¶„ì„ í”„ë¡¬í”„íŠ¸", "Please analyze the content type, main audience, tone, and suggest 3 improvements.")

UPLOAD_DIR = os.path.join(os.getcwd(), "uploaded")
os.makedirs(UPLOAD_DIR, exist_ok=True)

# Tesseract OCR ê²½ë¡œ ì„¤ì •
pytesseract.pytesseract.tesseract_cmd = r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe"

@st.cache_resource
def load_blip():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def describe_image_with_blip(pil_image):
    processor, model = load_blip()
    inputs = processor(pil_image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def extract_text_from_image(pil_image):
    return pytesseract.image_to_string(pil_image, lang='kor+eng')

def get_latest_wav_file():
    wav_files = sorted(
        glob.glob(os.path.join(UPLOAD_DIR, "*.wav")),
        key=os.path.getmtime,
        reverse=True
    )
    return wav_files[0] if wav_files else None

def safe_transcribe():
    filepath = get_latest_wav_file()
    if not filepath or not os.path.exists(filepath):
        raise FileNotFoundError(".wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    model = whisper.load_model("base")
    result = model.transcribe(filepath, fp16=torch.cuda.is_available(), language='ko')
    return result['text']

def extract_keyframes(video_path, fps=1):
    cap = cv2.VideoCapture(video_path)
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(original_fps * fps)
    frames = []
    count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        if count % interval == 0:
            path = os.path.join(UPLOAD_DIR, f"frame_{count}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
        count += 1
    cap.release()
    return frames

def analyze_with_ollama(prompt_text):
    template = PromptTemplate.from_template("{prompt_text}")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=prompt_text)

def summarize_all_inputs(frames_desc, transcript, title, prompt):
    summary = f"Title: {title}\n\n"
    summary += "Frame Descriptions:\n" + "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)]) + "\n\n"
    summary += f"Transcript:\n{transcript}\n\n"
    summary += prompt.strip()
    return summary

uploaded_video = st.file_uploader("ì˜ìƒ ì—…ë¡œë“œ", type=["mp4", "mov", "mkv"])
uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
uploaded_audio = st.file_uploader("ìŒì„± ì—…ë¡œë“œ", type=["wav", "mp3"])

if uploaded_image:
    image_obj = Image.open(uploaded_image).convert("RGB")
    st.image(image_obj, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_container_width=True)
    if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì´ë¯¸ì§€ ì„¤ëª… ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            desc = describe_image_with_blip(image_obj)
            extracted_text = extract_text_from_image(image_obj)

        image_name = uploaded_image.name

        refined_prompt = f"""ë‹¤ìŒì€ ê´‘ê³  ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•œ ì •ë³´ì…ë‹ˆë‹¤:

[íŒŒì¼ëª…]
{image_name}

[BLIP ì´ë¯¸ì§€ ì„¤ëª…]
{desc}

[OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸]
{extracted_text}

ğŸ” ì•„ë˜ í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•´ ê´‘ê³  ì „ë¬¸ê°€ë¡œì„œ ë¶„ì„ ë° ê°œì„ ì•ˆì„ ì œì‹œí•´ ì£¼ì„¸ìš”:

1. íŒŒì¼ëª…ìœ¼ë¡œ ì¶”ì • ê°€ëŠ¥í•œ ì—…ì¢…, ë¸Œëœë“œ, ì„œë¹„ìŠ¤, íƒ€ê²Ÿ ë“±
2. ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ, ê¸€ê¼´, ë ˆì´ì•„ì›ƒ êµ¬ì„±, í…ìŠ¤íŠ¸ í¬ê¸°/ë°°ì¹˜ì˜ ì „ëµì  ì˜ë¯¸
3. ì£¼ëª©ì„±ê³¼ CTA íš¨ê³¼ (ex. í˜œíƒ ë°›ê¸°, ì œí•œ ì¡°ê±´, ìœ ë„ í™”ì‚´í‘œ ë“±)
4. í†¤ì•¤ë§¤ë„ˆ (ì‹ ë¢°, ê±´ê°•, í™œê¸°, ê°ì„± ë“±) ë° ê°ì • ìœ ë„ ìš”ì†Œ
5. ê´‘ê³  ì‹¬ì‚¬ ê·œì • ìœ„ë°˜ ê°€ëŠ¥ì„± (ê³¼ì¥, ë¹„ì˜ë£Œì¸ ì‚¬ìš©, í‘œí˜„ ë“±)
6. ì‹œê° íë¦„(ìƒë‹¨ ê°•ì¡° â†’ í•˜ë‹¨ í´ë¦­ ìœ ë„ ë“±)ì˜ ì„¤ê³„ ì—¬ë¶€
7. ì „ì²´ì ìœ¼ë¡œ ì‹œì²­ìê°€ ì–´ë–¤ ì¸ì‹ì„ í•˜ê²Œ ë˜ëŠ”ì§€ ì˜ˆì¸¡

â†’ ë§ˆì§€ë§‰ìœ¼ë¡œ, í•´ë‹¹ ê´‘ê³ ë¥¼ ë³´ì™„/ê°œì„ í•˜ê¸° ìœ„í•œ ì‹¤ì§ˆì ì¸ ì‹¤í–‰ ì œì•ˆì„ 3ê°€ì§€ í•´ì£¼ì„¸ìš”.
"""

        with st.spinner("ê´‘ê³  ì „ë¬¸ê°€ ê´€ì  ë¶„ì„ ì¤‘..."):
            result = analyze_with_ollama(refined_prompt)

        st.success("âœ… ì´ë¯¸ì§€ ê´‘ê³  ë¶„ì„ ì™„ë£Œ")
        st.subheader("ğŸ§  ë¶„ì„ ê²°ê³¼")
        st.write(result)

if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
    st.video(video_path)
    if st.button("ì˜ìƒ ë¶„ì„ ì‹œì‘"):
        frames = extract_keyframes(video_path)
        descs = [describe_image_with_blip(Image.open(f)) for f in frames]
        audio_path = os.path.join(UPLOAD_DIR, "extracted_audio.wav")
        subprocess.run(["ffmpeg", "-y", "-i", video_path, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        transcript = safe_transcribe()
        final_prompt = summarize_all_inputs(descs, transcript, os.path.basename(video_path), prompt_text)
        result = analyze_with_ollama(final_prompt)
        st.subheader("ì˜ìƒ ë¶„ì„ ê²°ê³¼")
        st.write(result)

if uploaded_audio:
    suffix = os.path.splitext(uploaded_audio.name)[1]
    saved_path = os.path.join(UPLOAD_DIR, f"uploaded_audio{suffix}")
    with open(saved_path, "wb") as f:
        f.write(uploaded_audio.read())
    if st.button("ìŒì„± ë¶„ì„ ì‹œì‘"):
        transcript = safe_transcribe()
        result = analyze_with_ollama(f"Transcript:\n{transcript}\n\n{prompt_text}")
        st.subheader("ìŒì„± ë¶„ì„ ê²°ê³¼")
        st.code(transcript)
        st.write(result)

st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
