import streamlit as st
import os, tempfile, subprocess, torch, cv2
from PIL import Image
import whisper
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from datetime import datetime
from dotenv import load_dotenv
import re
from supabase import create_client

load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def parse_title_kor(filename):
    parts = filename.replace(".mp4", "").replace(".mov", "").replace(".mp3", "").replace(".wav", "").replace(".jpg", "").replace(".jpeg", "").replace(".png", "").split("-")
    return {
        "client": parts[0] if len(parts) > 0 else "ë¯¸ì§€ì •",
        "category": parts[1] if len(parts) > 1 else "ê¸°íƒ€",
        "subcontext": "-".join(parts[2:]) if len(parts) > 2 else ""
    }

def fetch_previous_summaries_by_category(category):
    try:
        result = supabase.table("analysis_results") \
            .select("summary_text") \
            .eq("category", category) \
            .order("created_at", desc=True) \
            .limit(5) \
            .execute()
        return [r["summary_text"] for r in result.data if r["summary_text"]]
    except Exception as e:
        print("ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:", e)
        return []

def fetch_experiences_by_category(category):
    try:
        result = supabase.table("performance_logs") \
            .select("experience, ctr") \
            .neq("experience", "") \
            .order("recorded_at", desc=True) \
            .limit(10) \
            .execute()
        return [f"- {r['experience']} (CTR: {r['ctr']}%)" for r in result.data if r["experience"]]
    except Exception as e:
        print("ê²½í—˜ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:", e)
        return []

def analyze_with_ollama(prompt_text, category=None):
    previous_summaries = fetch_previous_summaries_by_category(category) if category else []
    experiences = fetch_experiences_by_category(category) if category else []

    context_intro = "\n".join([f"- {s}" for s in previous_summaries])
    exp_intro = "\n".join(experiences)

    final_prompt = f"""ğŸ§  ì°¸ê³ í•  ì´ì „ {category} ê´‘ê³  ë¶„ì„ ìš”ì•½:
{context_intro}

ğŸ“Š ê´‘ê³  ì„±ê³¼ + ê²½í—˜:
{exp_intro}

ğŸ” ì§€ê¸ˆ ë¶„ì„í•  ì½˜í…ì¸ :
{prompt_text}
"""
    template = PromptTemplate.from_template("{prompt_text}")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=final_prompt)

def save_analysis_to_db(client_name, file_name, category, subcontext, summary, transcript, descriptions, prompt_text, content_type):
    supabase.table("analysis_results").insert({
        "client_name": client_name,
        "category": category,
        "subcontext": subcontext,
        "content_type": content_type,
        "file_name": file_name,
        "summary_text": summary,
        "raw_transcript": transcript,
        "frame_descriptions": descriptions,
        "prompt_used": prompt_text,
        "created_at": datetime.utcnow().isoformat()
    }).execute()

def save_performance_to_db(client_name, file_name, views, clicks, conversion, ctr, experience):
    supabase.table("performance_logs").insert({
        "client_name": client_name,
        "file_name": file_name,
        "views": views,
        "clicks": clicks,
        "conversion": conversion,
        "ctr": ctr,
        "experience": experience,
        "recorded_at": datetime.utcnow().isoformat()
    }).execute()

st.set_page_config(page_title="AI ê´‘ê³  ì „ëµ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ¯ ì‹œì˜¨ë§ˆì¼€íŒ… ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")

prompt_text = st.text_area("ë¶„ì„ í”„ë¡¬í”„íŠ¸", "ê´‘ê³  ì½˜í…ì¸ ì˜ íƒ€ê²Ÿ, í†¤, ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ê³  ì „ëµì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”.")

@st.cache_resource
def load_blip():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def describe_image_with_blip(pil_image):
    processor, model = load_blip()
    inputs = processor(pil_image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def transcribe_audio_whisper(audio_path):
    model = whisper.load_model("base")
    result = model.transcribe(audio_path, fp16=torch.cuda.is_available())
    return result['text']

def extract_keyframes(video_path, interval_sec=1):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(fps * interval_sec)
    frames, count = [], 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if count % interval == 0:
            path = os.path.join(tempfile.gettempdir(), f"frame_{count}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
        count += 1
    cap.release()
    return frames

def summarize_all_inputs(frames_desc, transcript, title, prompt):
    summary = f"ğŸ¬ ì½˜í…ì¸  ì œëª©: {title}\n\nğŸ–¼ï¸ í”„ë ˆì„ ì„¤ëª…:\n"
    summary += "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)])
    summary += f"\n\nğŸ“ í…ìŠ¤íŠ¸:\n{transcript}\n\nğŸ” ë¶„ì„ ì§€ì‹œ:\n{prompt.strip()}"
    return summary

uploaded_image = st.file_uploader("ğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
if uploaded_image:
    pil_image = Image.open(uploaded_image).convert("RGB")
    st.image(pil_image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_container_width=True)
    if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘..."):
            image_desc = describe_image_with_blip(pil_image)
        parsed = parse_title_kor(uploaded_image.name)
        client_name = parsed["client"]
        category = parsed["category"]
        subcontext = parsed["subcontext"]
        with st.spinner("Ollama ì „ëµ ë¶„ì„ ì¤‘..."):
            image_prompt = f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„¤ëª…: {image_desc}\n\n{prompt_text}"
            result = analyze_with_ollama(image_prompt, category)
        save_analysis_to_db(client_name, uploaded_image.name, category, subcontext, result, "", [image_desc], prompt_text, content_type="image")
        st.success("ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ âœ…")
        st.subheader("ğŸ§  ë¶„ì„ ê²°ê³¼")
        st.write(result)

uploaded_video = st.file_uploader("ğŸ¥ ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ", type=["mp4", "mov"])
if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
    st.video(video_path)
    if st.button("ì˜ìƒ ë¶„ì„ ì‹œì‘"):
        with st.spinner("í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."):
            frames = extract_keyframes(video_path)
            descriptions = [describe_image_with_blip(Image.open(f)) for f in frames]
        with st.spinner("ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘..."):
            audio_path = os.path.join(tempfile.gettempdir(), "audio.wav")
            subprocess.run(["ffmpeg", "-y", "-i", video_path, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            transcript = transcribe_audio_whisper(audio_path)
        parsed = parse_title_kor(uploaded_video.name)
        client_name = parsed["client"]
        category = parsed["category"]
        subcontext = parsed["subcontext"]
        with st.spinner("Ollama ë¶„ì„ ì¤‘..."):
            full_prompt = summarize_all_inputs(descriptions, transcript, uploaded_video.name, prompt_text)
            result = analyze_with_ollama(full_prompt, category)
        save_analysis_to_db(client_name, uploaded_video.name, category, subcontext, result, transcript, descriptions, prompt_text, content_type="video")
        st.success("ì˜ìƒ ë¶„ì„ ì™„ë£Œ âœ…")
        st.subheader("ğŸ§  ë¶„ì„ ê²°ê³¼")
        st.write(result)

st.markdown("---")
st.header("ğŸ“Š ê´‘ê³  ì„±ê³¼ ì…ë ¥")
with st.form("performance_form"):
    perf_file_name = st.text_input("íŒŒì¼ëª… (ì˜ˆ: í•˜ë‚˜ì¹˜ê³¼-ì„í”Œë€íŠ¸-í•œì§€ë°°ê²½.mp4)", "")
    parsed = parse_title_kor(perf_file_name)
    perf_client_name = parsed["client"]
    views = st.number_input("ì¡°íšŒìˆ˜", min_value=0)
    clicks = st.number_input("í´ë¦­ìˆ˜", min_value=0)
    conversion = st.number_input("ì „í™˜ìˆ˜", min_value=0)
    ctr = round((clicks / views) * 100, 2) if views else 0.0
    experience = st.text_area("ğŸ“ ê´‘ê³  ê²½í—˜ ë©”ëª¨", placeholder="ì˜ˆ: í•œì§€ ë°°ê²½ ë„£ì—ˆë”ë‹ˆ CTR ìƒìŠ¹")
    submitted = st.form_submit_button("ì„±ê³¼ + ê²½í—˜ ì €ì¥")
    if submitted and perf_file_name:
        save_performance_to_db(perf_client_name, perf_file_name, views, clicks, conversion, ctr, experience)
        st.success(f"{perf_client_name} ì„±ê³¼ + ê²½í—˜ ì €ì¥ ì™„ë£Œ âœ…")

st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
