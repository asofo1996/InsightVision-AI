import streamlit as st
import os, tempfile, cv2, torch, subprocess, glob
from PIL import Image
import whisper
import yt_dlp
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

st.set_page_config(page_title="AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ¬ AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")
prompt_text = st.text_area("ë¶„ì„ í”„ë¡¬í”„íŠ¸", "Please analyze the content type, main audience, tone, and suggest 3 improvements.")

@st.cache_resource
def load_blip():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def describe_image_with_blip(pil_image):
    processor, model = load_blip()
    inputs = processor(pil_image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def transcribe_audio_whisper():
    # temp í´ë” ë‚´ ê°€ì¥ ìµœê·¼ ìƒì„±ëœ .wav íŒŒì¼ì„ ì°¾ëŠ”ë‹¤
    temp_dir = tempfile.gettempdir()
    wav_files = sorted(
        glob.glob(os.path.join(temp_dir, "*.wav")),
        key=os.path.getmtime,
        reverse=True
    )
    if not wav_files:
        raise FileNotFoundError("âŒ .wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    latest_audio = wav_files[0]
    st.text(f"ğŸ§  Whisperê°€ ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼: {latest_audio}")
    model = whisper.load_model("base")
    result = model.transcribe(latest_audio, fp16=torch.cuda.is_available())
    return result['text']

def extract_keyframes(video_path, fps=1):
    cap = cv2.VideoCapture(video_path)
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(original_fps * fps)
    frames = []
    count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        if count % interval == 0:
            path = os.path.join(tempfile.gettempdir(), f"frame_{count}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
        count += 1
    cap.release()
    return frames

def analyze_with_ollama(prompt_text):
    template = PromptTemplate.from_template("{prompt_text}")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=prompt_text)

def summarize_all_inputs(frames_desc, transcript, title, prompt):
    summary = f"Title: {title}\n\n"
    summary += "Frame Descriptions:\n" + "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)]) + "\n\n"
    summary += f"Transcript:\n{transcript}\n\n"
    summary += prompt.strip()
    return summary

def download_youtube_audio(url):
    output_path = os.path.join(tempfile.gettempdir(), "youtube_audio.%(ext)s")
    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': output_path,
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'wav',
        }],
        'quiet': True,
        'noplaylist': True
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
    # Whisperì—ì„œ ìë™ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ .wav íŒŒì¼ì„ ì°¾ì„ ê²ƒì´ë¯€ë¡œ ë³„ë„ ë¦¬í„´ ë¶ˆí•„ìš”
    return True

# ì—…ë¡œë“œ í•­ëª©ë“¤
uploaded_video = st.file_uploader("ğŸ¥ ì˜ìƒ ì—…ë¡œë“œ", type=["mp4", "mov", "mkv"])
uploaded_image = st.file_uploader("ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
uploaded_audio = st.file_uploader("ğŸ§ ìŒì„± ì—…ë¡œë“œ", type=["wav", "mp3"])

if uploaded_image:
    image = Image.open(uploaded_image).convert("RGB")
    st.image(image, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_container_width=True)
    if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘"):
        desc = describe_image_with_blip(image)
        result = analyze_with_ollama(f"Image:\n{desc}\n\n{prompt_text}")
        st.subheader("ğŸ“Œ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
        st.write(result)

if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
        st.video(video_path)

        if st.button("ì˜ìƒ ë¶„ì„ ì‹œì‘"):
            with st.spinner("ğŸ í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."):
                frames = extract_keyframes(video_path)
                descs = [describe_image_with_blip(Image.open(f)) for f in frames]

            with st.spinner("ğŸ§ Whisper ìŒì„± ë¶„ì„ ì¤‘..."):
                extracted_audio = os.path.join(tempfile.gettempdir(), "from_video.wav")
                subprocess.run([
                    "ffmpeg", "-y", "-i", video_path,
                    "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", extracted_audio
                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                transcript = transcribe_audio_whisper()

            with st.spinner("ğŸ§  Ollama ë¶„ì„ ì¤‘..."):
                prompt = summarize_all_inputs(descs, transcript, os.path.basename(video_path), prompt_text)
                result = analyze_with_ollama(prompt)
                st.subheader("ğŸ¬ ì˜ìƒ ë¶„ì„ ê²°ê³¼")
                st.write(result)

if uploaded_audio:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(uploaded_audio.read())
    if st.button("ìŒì„± íŒŒì¼ ë¶„ì„ ì‹œì‘"):
        transcript = transcribe_audio_whisper()
        result = analyze_with_ollama(f"Transcript:\n{transcript}\n\n{prompt_text}")
        st.subheader("ğŸ§¾ ìŒì„± ë¶„ì„ ê²°ê³¼")
        st.code(transcript)
        st.write(result)

# ìœ íŠœë¸Œ or ë¡œì»¬ ë¶„ì„
st.markdown("---")
st.subheader("ğŸ”— ìœ íŠœë¸Œ ë§í¬ ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ë¶„ì„")
col1, col2 = st.columns([1, 3])
with col1:
    mode = st.radio("ë¶„ì„ ë°©ì‹", ["ìœ íŠœë¸Œ ë§í¬", "ë¡œì»¬ ì˜¤ë””ì˜¤"], horizontal=True)
with col2:
    user_input = st.text_input("ë§í¬ ë˜ëŠ” ê²½ë¡œ ì…ë ¥")

if st.button("ì˜¤ë””ì˜¤ ìš”ì•½ ë¶„ì„ ì‹œì‘"):
    try:
        if mode == "ìœ íŠœë¸Œ ë§í¬":
            with st.spinner("ğŸ“¥ ìœ íŠœë¸Œ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
                download_youtube_audio(user_input)
        else:
            if not os.path.exists(user_input):
                raise FileNotFoundError("âŒ ì…ë ¥ ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            # ë¡œì»¬ ë³µì‚¬í•˜ì—¬ í…œí”„ì— ë„£ê¸°
            tmp_path = os.path.join(tempfile.gettempdir(), os.path.basename(user_input))
            with open(user_input, 'rb') as src, open(tmp_path, 'wb') as dst:
                dst.write(src.read())

        with st.spinner("ğŸ§  Whisper ë¶„ì„ ì¤‘..."):
            transcript = transcribe_audio_whisper()

        with st.spinner("ğŸ§  Ollama ìš”ì•½ ì¤‘..."):
            result = analyze_with_ollama(f"Transcript:\n{transcript}\n\n{prompt_text}")
            st.success("âœ… ë¶„ì„ ì™„ë£Œ")
            st.code(transcript)
            st.write(result)

    except Exception as e:
        st.error(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
