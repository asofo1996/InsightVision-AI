import streamlit as st
import os
import shutil
import glob
import cv2
import tempfile
import subprocess
import torch
import torchaudio
import whisper
import yt_dlp
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ¬ AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")
prompt_text = st.text_area("ë¶„ì„ í”„ë¡¬í”„íŠ¸", "Please analyze the content type, main audience, tone, and suggest 3 improvements.")

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ì„¤ì •
UPLOAD_DIR = os.path.join(os.getcwd(), "uploaded")
os.makedirs(UPLOAD_DIR, exist_ok=True)

# --- ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©) ---
@st.cache_resource
def load_blip():
    """BLIP ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ"""
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def describe_image_with_blip(pil_image):
    """BLIP ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±"""
    processor, model = load_blip()
    inputs = processor(pil_image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def get_latest_wav_file():
    """ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ì— ìˆ˜ì •ëœ .wav íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    wav_files = sorted(
        glob.glob(os.path.join(UPLOAD_DIR, "*.wav")),
        key=os.path.getmtime,
        reverse=True
    )
    return wav_files[0] if wav_files else None

def safe_transcribe():
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Whisper ì‚¬ìš©)"""
    filepath = get_latest_wav_file()
    if not filepath or not os.path.exists(filepath):
        raise FileNotFoundError(".wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    st.info(f"ğŸ§  ë¶„ì„ ëŒ€ìƒ ì˜¤ë””ì˜¤ íŒŒì¼: {filepath}")
    
    # torchaudioë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ ë° ë¦¬ìƒ˜í”Œë§
    waveform, sample_rate = torchaudio.load(filepath)
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)
        
    audio = waveform.squeeze().numpy()
    
    # Whisper ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    model = whisper.load_model("base")
    result = model.transcribe(audio, fp16=torch.cuda.is_available(), language='ko')
    return result['text']

def extract_keyframes(video_path, fps=1):
    """ë¹„ë””ì˜¤ì—ì„œ ì´ˆë‹¹ 1í”„ë ˆì„ì”© í‚¤í”„ë ˆì„ ì¶”ì¶œ"""
    cap = cv2.VideoCapture(video_path)
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(original_fps / fps) if original_fps > 0 else 1
    frames = []
    count = 0
    frame_number = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if count % interval == 0:
            path = os.path.join(UPLOAD_DIR, f"frame_{frame_number}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
            frame_number += 1
        count += 1
    cap.release()
    return frames

def analyze_with_ollama(prompt_text):
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰"""
    template = PromptTemplate.from_template("{prompt_text}")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=prompt_text)

def summarize_all_inputs(frames_desc, transcript, title, prompt):
    """ëª¨ë“  ì…ë ¥(í”„ë ˆì„ ì„¤ëª…, ëŒ€ë³¸, ì œëª©, í”„ë¡¬í”„íŠ¸)ì„ í•˜ë‚˜ì˜ ìš”ì•½ í…ìŠ¤íŠ¸ë¡œ ê²°í•©"""
    summary = f"Title: {title}\n\n"
    summary += "Frame Descriptions:\n" + "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)]) + "\n\n"
    summary += f"Transcript:\n{transcript}\n\n"
    summary += prompt.strip()
    return summary

def download_youtube_audio(url):
    """yt-dlpë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë¸Œ ì˜¤ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  wavë¡œ ë³€í™˜"""
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (í™•ì¥ì ì œì™¸)
    out_base = os.path.join(UPLOAD_DIR, "youtube_audio")
    
    ydl_opts = {
        'format': 'bestaudio/best',
        # --- ğŸ’¥ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ğŸ’¥ ---
        # yt-dlpê°€ í›„ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì˜¬ë°”ë¥¸ í™•ì¥ì(.wav)ë¥¼ ë¶™ì´ë„ë¡ í™•ì¥ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        'outtmpl': out_base,
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'wav',
        }],
        'quiet': True,
        'noplaylist': True
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])

# --- Streamlit UI êµ¬ì„± ---

# 1. íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
st.subheader("íŒŒì¼ ì§ì ‘ ì—…ë¡œë“œí•˜ì—¬ ë¶„ì„")
uploaded_video = st.file_uploader("ì˜ìƒ ì—…ë¡œë“œ", type=["mp4", "mov", "mkv"])
uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
uploaded_audio = st.file_uploader("ìŒì„± ì—…ë¡œë“œ", type=["wav", "mp3"])

# ì´ë¯¸ì§€ ë¶„ì„
if uploaded_image:
    image = Image.open(uploaded_image).convert("RGB")
    st.image(image, caption="ì—…ë¡œë“œ ì´ë¯¸ì§€")
    if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì´ë¯¸ì§€ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            desc = describe_image_with_blip(image)
            final_prompt = f"Analyze the following image description:\n{desc}\n\nUser's Request:\n{prompt_text}"
            result = analyze_with_ollama(final_prompt)
            st.subheader("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
            st.write(result)

# ì˜ìƒ ë¶„ì„
if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_video.name)[1]) as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
    st.video(video_path)
    if st.button("ì˜ìƒ ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì˜ìƒì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. í‚¤í”„ë ˆì„ì„ ì¶”ì¶œí•˜ê³  ì˜¤ë””ì˜¤ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤..."):
            frames = extract_keyframes(video_path)
            descs = [describe_image_with_blip(Image.open(f)) for f in frames]
            
            # ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
            audio_path = os.path.join(UPLOAD_DIR, "extracted_audio.wav")
            subprocess.run([
                "ffmpeg", "-y", "-i", video_path, "-vn", 
                "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path
            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            transcript = safe_transcribe()
            final_prompt = summarize_all_inputs(descs, transcript, os.path.basename(uploaded_video.name), prompt_text)
            
            st.info("LLM ëª¨ë¸ë¡œ ì¢…í•© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = analyze_with_ollama(final_prompt)
            st.subheader("ğŸ¥ ì˜ìƒ ë¶„ì„ ê²°ê³¼")
            st.write(result)

# ìŒì„± ë¶„ì„
if uploaded_audio:
    suffix = os.path.splitext(uploaded_audio.name)[1]
    saved_path = os.path.join(UPLOAD_DIR, f"uploaded_audio{suffix}")
    with open(saved_path, "wb") as f:
        f.write(uploaded_audio.read())
    st.audio(saved_path)
    if st.button("ìŒì„± ë¶„ì„ ì‹œì‘"):
        with st.spinner("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            transcript = safe_transcribe()
            final_prompt = f"Analyze the following transcript:\n{transcript}\n\nUser's Request:\n{prompt_text}"
            result = analyze_with_ollama(final_prompt)
            st.subheader("ğŸ”Š ìŒì„± ë¶„ì„ ê²°ê³¼")
            st.code(transcript)
            st.write(result)

st.markdown("---")

# 2. ìœ íŠœë¸Œ ë§í¬ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ ë¶„ì„ ì„¹ì…˜
st.subheader("ìœ íŠœë¸Œ ë§í¬ ë˜ëŠ” ë¡œì»¬ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¡œ ë¶„ì„")
mode = st.radio("ë°©ì‹ ì„ íƒ", ["ìœ íŠœë¸Œ ë§í¬", "ë¡œì»¬ íŒŒì¼"], horizontal=True)
user_input = st.text_input("ë§í¬ ë˜ëŠ” ì „ì²´ íŒŒì¼ ê²½ë¡œ ì…ë ¥")

if st.button("ì˜¤ë””ì˜¤ ìš”ì•½ ë¶„ì„ ì‹œì‘"):
    try:
        with st.spinner("ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ê³  ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤..."):
            if mode == "ìœ íŠœë¸Œ ë§í¬":
                if not user_input.startswith("http"):
                    st.error("âŒ ìœ íš¨í•œ ìœ íŠœë¸Œ ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: https://www.youtube.com/watch?v=...)")
                else:
                    download_youtube_audio(user_input)
            else: # ë¡œì»¬ íŒŒì¼ ëª¨ë“œ
                if not os.path.exists(user_input):
                    raise FileNotFoundError(f"âŒ í•´ë‹¹ ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {user_input}")
                dst = os.path.join(UPLOAD_DIR, os.path.basename(user_input))
                shutil.copyfile(user_input, dst)

            # ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ë³µì‚¬ í›„ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ë¶„ì„
            transcript = safe_transcribe()
            final_prompt = f"Analyze the following transcript:\n{transcript}\n\nUser's Request:\n{prompt_text}"
            result = analyze_with_ollama(final_prompt)
            
            st.success("âœ… ë¶„ì„ ì™„ë£Œ")
            st.subheader("ğŸ§ ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼")
            st.code(transcript)
            st.write(result)

    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
