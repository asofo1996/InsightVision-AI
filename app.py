import streamlit as st
import os, tempfile, subprocess, torch, cv2
from PIL import Image
import whisper
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from datetime import datetime
from dotenv import load_dotenv
import re
from supabase import create_client

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# âœ… íŒŒì¼ëª… ê¸°ë°˜ ìë™ ë¶„ë¥˜ í•¨ìˆ˜ (í•œê¸€/ê´„í˜¸ í¬í•¨ êµ¬ì¡° ëŒ€ì‘)
def parse_title_kor(filename):
    filename = os.path.splitext(filename)[0]
    filename = filename.replace("(", "_").replace(")", "_")
    parts = re.split(r"[_\-]+", filename)
    client = parts[0] if len(parts) > 0 else "ë¯¸ì§€ì •"
    category = parts[1] if len(parts) > 1 else "ê¸°íƒ€"
    subcontext = "_".join(parts[2:]) if len(parts) > 2 else ""
    return {
        "client": client,
        "category": category,
        "subcontext": subcontext
    }

# âœ… DBì—ì„œ ê¸°ì¡´ ìš”ì•½ ë° ê²½í—˜ ë¶ˆëŸ¬ì˜¤ê¸°
def fetch_previous_summaries_by_category(category):
    try:
        result = supabase.table("analysis_results") \
            .select("summary_text") \
            .eq("category", category) \
            .order("created_at", desc=True) \
            .limit(5) \
            .execute()
        return [r["summary_text"] for r in result.data if r["summary_text"]]
    except Exception as e:
        print("ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:", e)
        return []

def fetch_experiences_by_category(category):
    try:
        result = supabase.table("performance_logs") \
            .select("experience, ctr") \
            .neq("experience", "") \
            .order("recorded_at", desc=True) \
            .limit(10) \
            .execute()
        return [f"- {r['experience']} (CTR: {r['ctr']}%)" for r in result.data if r["experience"]]
    except Exception as e:
        print("ê²½í—˜ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:", e)
        return []

# âœ… ì „ëµ ë¶„ì„ ìš”ì²­
def analyze_with_ollama(prompt_text, category=None):
    previous_summaries = fetch_previous_summaries_by_category(category) if category else []
    experiences = fetch_experiences_by_category(category) if category else []

    context_intro = "\n".join([f"- {s}" for s in previous_summaries])
    exp_intro = "\n".join(experiences)

    full_prompt = f"""[ë¶„ì„ ìš”ì²­ ëª©ì ]
1. ê´‘ê³  ì½˜í…ì¸ ê°€ ì—…ì¢…ê³¼ ì¢…ëª©ì— ì „ëµì ìœ¼ë¡œ ì í•©í•œì§€ íŒë‹¨í•´ ì£¼ì„¸ìš”.
2. í˜„ì¬ êµ­ë‚´ íƒ€ê²Ÿ ì‹œì¥ ë° ì†Œë¹„ì íŠ¹ì„±ê³¼ ë¹„êµí–ˆì„ ë•Œ íƒ€ê²Ÿ ì •í•©ì„±ì´ ë†’ì€ì§€ í‰ê°€í•´ ì£¼ì„¸ìš”.
3. í´ë¦­ë¥ ê³¼ ì „í™˜ìœ¨ì„ ë†’ì´ê¸° ìœ„í•œ ì½˜í…ì¸  êµ¬ì„± ìš”ì†Œê°€ ì˜ ì‘ë™í•˜ëŠ”ì§€ ë¶„ì„í•´ ì£¼ì„¸ìš”.
4. ì‹¤ë¬´ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì „ëµì„ 3ê°€ì§€ ì´ìƒ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•´ ì£¼ì„¸ìš”.

[ì½˜í…ì¸  ì •ë³´]
- ê³¼ê±° ê´‘ê³  ë¶„ì„ ìš”ì•½ ({category}):
{context_intro}

- ê´‘ê³  ì„±ê³¼ ë° ê²½í—˜:
{exp_intro}

- í˜„ì¬ ì½˜í…ì¸  ìš”ì•½:
{prompt_text}
"""

    print("\n---ğŸ” Ollama Prompt Input ---\n", full_prompt)

    template = PromptTemplate.from_template("{prompt_text}")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=full_prompt)

# âœ… Streamlit UI êµ¬ì„± ì‹œì‘
st.set_page_config(page_title="AI ê´‘ê³  ì „ëµ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ¯ ì‹œì˜¨ë§ˆì¼€íŒ… ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")

prompt_text = st.text_area("ë¶„ì„ í”„ë¡¬í”„íŠ¸", "ê´‘ê³  ì½˜í…ì¸ ê°€ ì—…ì¢…Â·íƒ€ê²ŸÂ·ì „í™˜ ì „ëµ ì¸¡ë©´ì—ì„œ ì‹¤ë¬´ì— ì í•©í•œì§€ ì •ë°€ ë¶„ì„í•˜ê³ , êµ¬ì²´ì ì¸ ë§ˆì¼€íŒ… ê°œì„ ì•ˆì„ 3ê°€ì§€ ì´ìƒ ì œì‹œí•´ ì£¼ì„¸ìš”.")
