import streamlit as st
import os, tempfile, subprocess, torch, cv2
from PIL import Image
import whisper
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# ì•± ì„¤ì •
st.set_page_config(page_title="ì‹œì˜¨ë§ˆì¼€íŒ… ì½˜í…ì¸  ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ¯ ì‹œì˜¨ë§ˆì¼€íŒ… AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("---")

# ì‚¬ìš©ì ì…ë ¥
prompt_text = st.text_area("ë¶„ì„ í”„ë¡¬í”„íŠ¸", 
    "Please analyze the content type, main audience, tone, and suggest 3 improvements.", key="main_prompt")

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_resource
def load_blip():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def describe_image_with_blip(pil_image):
    processor, model = load_blip()
    inputs = processor(pil_image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def transcribe_audio_whisper(audio_path):
    model = whisper.load_model("base")
    result = model.transcribe(audio_path, fp16=torch.cuda.is_available())
    return result['text']

def extract_keyframes(video_path, interval_sec=1):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(fps * interval_sec)
    frames, count = [], 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if count % interval == 0:
            path = os.path.join(tempfile.gettempdir(), f"frame_{count}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
        count += 1
    cap.release()
    return frames

def analyze_with_ollama(prompt_text):
    template = PromptTemplate.from_template("{prompt_text}")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=prompt_text)

def summarize_all_inputs(frames_desc, transcript, title, prompt):
    summary = f"ğŸ¬ ì˜ìƒ ì œëª©: {title}\n\nğŸ–¼ï¸ í”„ë ˆì„ ì„¤ëª…:\n"
    summary += "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)])
    summary += f"\n\nğŸ“ ìŒì„± í…ìŠ¤íŠ¸:\n{transcript}\n\nğŸ” ë¶„ì„ ì§€ì‹œ:\n{prompt.strip()}"
    return summary

# ì—…ë¡œë“œ ìš”ì†Œ
uploaded_video = st.file_uploader("ğŸ“½ï¸ ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ", type=["mp4", "mov"], key="video_upload")
uploaded_image = st.file_uploader("ğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"], key="image_upload")
uploaded_audio = st.file_uploader("ğŸ§ ìŒì„± íŒŒì¼ ì—…ë¡œë“œ", type=["mp3", "wav"], key="audio_upload")

# ì´ë¯¸ì§€ ë¶„ì„
if uploaded_image:
    image_obj = Image.open(uploaded_image).convert("RGB")
    st.image(image_obj, caption="ì—…ë¡œë“œ ì´ë¯¸ì§€", use_container_width=True)
    if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘", key="start_image_analysis"):
        with st.spinner("ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘..."):
            desc = describe_image_with_blip(image_obj)
        with st.spinner("Ollama ë¶„ì„ ì¤‘..."):
            result = analyze_with_ollama(f"ì‹œì˜¨ë§ˆì¼€íŒ… ê´‘ê³  ì „ë¬¸ê°€ ê¸°ì¤€ ë¶„ì„\níŒŒì¼ëª…: {uploaded_image.name}\nì´ë¯¸ì§€ ì„¤ëª…: {desc}\n\n{prompt_text}")
        st.success("ë¶„ì„ ì™„ë£Œ âœ…")
        st.write(result)

# ì˜ìƒ ë¶„ì„
if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
    st.video(video_path)
    if st.button("ì˜ìƒ ë¶„ì„ ì‹œì‘", key="start_video_analysis"):
        with st.spinner("ğŸ“¸ í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."):
            frames = extract_keyframes(video_path)
            descriptions = [describe_image_with_blip(Image.open(f)) for f in frames]

        with st.spinner("ğŸ—£ï¸ Whisper ìŒì„± ë³€í™˜ ì¤‘..."):
            audio_path = os.path.join(tempfile.gettempdir(), "audio.wav")
            subprocess.run(["ffmpeg", "-y", "-i", video_path, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path],
                           stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            transcript = transcribe_audio_whisper(audio_path)

        with st.spinner("ğŸ§  Ollama ë¶„ì„ ì¤‘..."):
            final_prompt = summarize_all_inputs(descriptions, transcript, os.path.basename(video_path), prompt_text)
            result = analyze_with_ollama(final_prompt)
            st.success("ì˜ìƒ ë¶„ì„ ì™„ë£Œ âœ…")
            st.subheader("ğŸ§  ë¶„ì„ ê²°ê³¼")
            st.write(result)

# ìŒì„± ë¶„ì„
if uploaded_audio:
    suffix = ".mp3" if uploaded_audio.name.endswith(".mp3") else ".wav"
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(uploaded_audio.read())
        audio_path = tmp.name
    if st.button("ìŒì„± ë¶„ì„ ì‹œì‘", key="start_audio_analysis"):
        if audio_path.endswith(".mp3"):
            converted_path = audio_path.replace(".mp3", ".wav")
            subprocess.run(["ffmpeg", "-y", "-i", audio_path, converted_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            audio_path = converted_path
        with st.spinner("Whisper í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘..."):
            transcript = transcribe_audio_whisper(audio_path)
        with st.spinner("Ollama ë¶„ì„ ì¤‘..."):
            result = analyze_with_ollama(f"Transcript:\n{transcript}\n\n{prompt_text}")
        st.success("ìŒì„± ë¶„ì„ ì™„ë£Œ âœ…")
        st.write("ì „ì²´ í…ìŠ¤íŠ¸:")
        st.code(transcript)
        st.write("ìš”ì•½ ê²°ê³¼:")
        st.write(result)

# í‘¸í„°
st.markdown("---")
st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
