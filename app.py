import streamlit as st
import os, tempfile, cv2, subprocess, torch, time
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import whisper
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

st.set_page_config(page_title="AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")
st.title("AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")

prompt_text = st.text_area("ë¶„ì„ ìš”ì²­ ë¬¸ì¥", "Please analyze the content type, main audience, tone, and suggest 3 improvements.")

@st.cache_resource
def load_blip():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base", use_fast=True)
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

@st.cache_resource
def load_whisper():
    return whisper.load_model("tiny")  # base â†’ tiny ë¡œ ì „í™˜ (ì†ë„ í–¥ìƒ)

def describe_image_with_blip(pil_image):
    processor, model = load_blip()
    inputs = processor(pil_image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def extract_audio_ffmpeg(video_path):
    audio_path = os.path.join(tempfile.gettempdir(), "audio.wav")
    command = [
        "ffmpeg", "-y", "-i", video_path,
        "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path
    ]
    result = subprocess.run(command, capture_output=True, text=True)

    print("ğŸ“¤ ffmpeg ëª…ë ¹ì–´:", ' '.join(command))
    print("ğŸ“¥ ffmpeg stdout:", result.stdout)
    print("ğŸ“¥ ffmpeg stderr:", result.stderr)

    if result.returncode != 0:
        st.error("âŒ ffmpeg ì‹¤í–‰ ì‹¤íŒ¨! stderrë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None
    if not os.path.exists(audio_path) or os.path.getsize(audio_path) < 1000:
        st.error("âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (audio.wav ì—†ìŒ ë˜ëŠ” 1KB ì´í•˜).")
        return None
    return audio_path

def transcribe_audio_whisper(audio_path):
    model = load_whisper()
    st.info("ğŸŸ¡ Whisper ì „ì‚¬ ì‹œì‘ë¨ - ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...")
    start = time.time()
    result = model.transcribe(audio_path, fp16=torch.cuda.is_available())
    end = time.time()
    st.success(f"ğŸŸ¢ Whisper ì „ì‚¬ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {int(end - start)}ì´ˆ)")
    return result['text']

def extract_all_keyframes(video_path, interval_sec=2):
    cap = cv2.VideoCapture(video_path)
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(original_fps * interval_sec)
    frames = []
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % frame_interval == 0:
            path = os.path.join(tempfile.gettempdir(), f"frame_{frame_count}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
        frame_count += 1
    cap.release()
    return frames

def summarize_video_inputs(frames_desc, transcript, title, prompt):
    summary = f"Title: {title}\n\n"
    summary += "Frame Descriptions:\n" + "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)]) + "\n\n"
    summary += f"Transcript:\n{transcript}\n\n"
    summary += prompt.strip()
    return summary

def analyze_with_ollama(prompt_text):
    template = PromptTemplate.from_template("""{prompt_text}""")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=prompt_text)

video_path = None
image_obj = None

uploaded_video = st.file_uploader("ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ", type=["mp4"], key="upload_video")
if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
        st.video(video_path)

uploaded_image = st.file_uploader("ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"], key="upload_image")
if uploaded_image:
    image_obj = Image.open(uploaded_image).convert("RGB")
    st.image(image_obj, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°", use_container_width=True)

col1, col2 = st.columns(2)

with col1:
    if image_obj is not None:
        if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘"):
            with st.spinner("ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘..."):
                desc = describe_image_with_blip(image_obj)
            with st.spinner("Ollama ë¶„ì„ ì¤‘..."):
                result = analyze_with_ollama(f"Image Description:\n{desc}\n\n{prompt_text}")
            st.success("ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ")
            st.subheader("ë¶„ì„ ê²°ê³¼")
            st.write(result)

with col2:
    if video_path is not None:
        if st.button("ì˜ìƒ ë¶„ì„ ì‹œì‘"):
            total_steps = 3
            overall_progress = st.progress(0)
            status_text = st.empty()

            t0 = time.perf_counter()
            with st.spinner("í”„ë ˆì„ ì¶”ì¶œ ì¤‘ (2ì´ˆ ê°„ê²©)..."):
                frames = extract_all_keyframes(video_path)
                descriptions = []
                for i, f in enumerate(frames):
                    desc = describe_image_with_blip(Image.open(f))
                    descriptions.append(desc)
                    progress = (i + 1) / len(frames) * (1 / total_steps)
                    overall_progress.progress(progress)
                    status_text.write(f"í”„ë ˆì„ ë¶„ì„ ì§„í–‰ë¥ : {int(progress * 100)}%")

            status_text.write("í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ")
            overall_progress.progress(1 / total_steps)

            with st.spinner("Whisperë¥¼ í†µí•œ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘..."):
                audio_path = extract_audio_ffmpeg(video_path)
                if audio_path is None:
                    st.stop()
                st.audio(audio_path)
                transcript = transcribe_audio_whisper(audio_path)
                overall_progress.progress(2 / total_steps)
                status_text.write("ìŒì„± ì „ì‚¬ ì™„ë£Œ")

            with st.spinner("Ollama ì¢…í•© ë¶„ì„ ì¤‘..."):
                title = os.path.basename(video_path)
                final_prompt = summarize_video_inputs(descriptions, transcript, title, prompt_text)
                result = analyze_with_ollama(final_prompt)

            overall_progress.progress(1.0)
            status_text.write("ë¶„ì„ ì™„ë£Œ")
            t1 = time.perf_counter()

            st.success(f"ì˜ìƒ ë¶„ì„ ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {int(t1 - t0)}ì´ˆ)")
            st.subheader("ë¶„ì„ ê²°ê³¼")
            st.write(result)

if image_obj is None and video_path is None:
    st.warning("ì˜ìƒ ë˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")

st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
