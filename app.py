import streamlit as st
import os
import shutil
import glob
import cv2
import tempfile
import subprocess
import torch
import torchaudio
import whisper
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

st.set_page_config(page_title="AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ¬ AI ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")
prompt_text = st.text_area("ë¶„ì„ í”„ë¡¬í”„íŠ¸", "Please analyze the content type, main audience, tone, and suggest 3 improvements.")

UPLOAD_DIR = os.path.join(os.getcwd(), "uploaded")
os.makedirs(UPLOAD_DIR, exist_ok=True)

@st.cache_resource
def load_blip():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def describe_image_with_blip(pil_image):
    processor, model = load_blip()
    inputs = processor(pil_image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

def get_latest_wav_file():
    wav_files = sorted(
        glob.glob(os.path.join(UPLOAD_DIR, "*.wav")),
        key=os.path.getmtime,
        reverse=True
    )
    return wav_files[0] if wav_files else None

def safe_transcribe():
    filepath = get_latest_wav_file()
    if not filepath or not os.path.exists(filepath):
        raise FileNotFoundError(".wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    st.info(f"ğŸ§  ë¶„ì„ ëŒ€ìƒ íŒŒì¼: {filepath}")
    waveform, sample_rate = torchaudio.load(filepath)
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)
    audio = waveform.squeeze().numpy()
    model = whisper.load_model("base")
    result = model.transcribe(audio, fp16=torch.cuda.is_available(), language='ko')
    return result['text']

def extract_keyframes(video_path, fps=1):
    cap = cv2.VideoCapture(video_path)
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(original_fps * fps)
    frames = []
    count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        if count % interval == 0:
            path = os.path.join(UPLOAD_DIR, f"frame_{count}.jpg")
            cv2.imwrite(path, frame)
            frames.append(path)
        count += 1
    cap.release()
    return frames

def analyze_with_ollama(prompt_text):
    template = PromptTemplate.from_template("{prompt_text}")
    llm = Ollama(model="llama3")
    chain = LLMChain(prompt=template, llm=llm)
    return chain.run(prompt_text=prompt_text)

def summarize_all_inputs(frames_desc, transcript, title, prompt):
    summary = f"Title: {title}\n\n"
    summary += "Frame Descriptions:\n" + "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(frames_desc)]) + "\n\n"
    summary += f"Transcript:\n{transcript}\n\n"
    summary += prompt.strip()
    return summary

# ì—…ë¡œë“œ
uploaded_video = st.file_uploader("ì˜ìƒ ì—…ë¡œë“œ", type=["mp4", "mov", "mkv"])
uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
uploaded_audio = st.file_uploader("ìŒì„± ì—…ë¡œë“œ", type=["wav", "mp3"])

if uploaded_image:
    image_obj = Image.open(uploaded_image).convert("RGB")
    st.image(image_obj, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_container_width=True)
    if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘..."):
            desc = describe_image_with_blip(image_obj)

        image_name = uploaded_image.name
        refined_prompt = f"""ì•„ë˜ëŠ” BLIP ëª¨ë¸ì´ ìƒì„±í•œ ì´ë¯¸ì§€ ì„¤ëª…ì…ë‹ˆë‹¤:

{desc}

ì´ ì´ë¯¸ì§€ëŠ” ê´‘ê³  ì´ë¯¸ì§€ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ í•­ëª©ì„ ë°”íƒ•ìœ¼ë¡œ ê´‘ê³  ì „ë¬¸ê°€ë¡œì„œ ì´ë¯¸ì§€ ë¶„ì„ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”:
1. ì´ë¯¸ì§€ì˜ íŒŒì¼ëª…: {image_name}
2. ì´ë¯¸ì§€ì— ì‚¬ìš©ëœ ì£¼ìš” ìƒ‰ìƒ, ë°°ê²½, í…ìŠ¤íŠ¸, êµ¬ì„± ìš”ì†Œ
3. ì´ë¯¸ì§€ ë‚´ ë¬¸êµ¬(í…ìŠ¤íŠ¸)ì˜ ì „ë‹¬ë ¥ê³¼ ë¸Œëœë“œ ì „ë‹¬ íš¨ê³¼
4. ë ˆì´ì•„ì›ƒ êµ¬ì„±ì˜ ì‹œê°ì  ì™„ì„±ë„
5. íƒ€ê²Ÿ ì†Œë¹„ìì¸µê³¼ì˜ ì í•©ì„±
6. SNS, ë””ìŠ¤í”Œë ˆì´ ê´‘ê³ , ë°°ë„ˆ ê´‘ê³  ë“±ì˜ í™œìš© ì í•©ì„±
7. ê´‘ê³  ì‹¬ì‚¬ë¥¼ í†µê³¼í•˜ê¸° ìœ„í•œ ë¬¸êµ¬/êµ¬ì„± ê°œì„  í¬ì¸íŠ¸
8. ì „ë°˜ì ì¸ Tone & Mannerì— ëŒ€í•œ í‰ê°€

ìœ„ ë¶„ì„ í›„, ê°œì„  ì•„ì´ë””ì–´ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ ì£¼ì„¸ìš”.
"""

        with st.spinner("Ollama ê´‘ê³  ë¶„ì„ ì¤‘..."):
            result = analyze_with_ollama(refined_prompt)

        st.success("âœ… ì´ë¯¸ì§€ ê´‘ê³  ë¶„ì„ ì™„ë£Œ")
        st.subheader("ğŸ§  ë¶„ì„ ê²°ê³¼")
        st.write(result)

if uploaded_video:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded_video.read())
        video_path = tmp.name
    st.video(video_path)
    if st.button("ì˜ìƒ ë¶„ì„ ì‹œì‘"):
        frames = extract_keyframes(video_path)
        descs = [describe_image_with_blip(Image.open(f)) for f in frames]
        audio_path = os.path.join(UPLOAD_DIR, "extracted_audio.wav")
        subprocess.run(["ffmpeg", "-y", "-i", video_path, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        transcript = safe_transcribe()
        final_prompt = summarize_all_inputs(descs, transcript, os.path.basename(video_path), prompt_text)
        result = analyze_with_ollama(final_prompt)
        st.subheader("ì˜ìƒ ë¶„ì„ ê²°ê³¼")
        st.write(result)

if uploaded_audio:
    suffix = os.path.splitext(uploaded_audio.name)[1]
    saved_path = os.path.join(UPLOAD_DIR, f"uploaded_audio{suffix}")
    with open(saved_path, "wb") as f:
        f.write(uploaded_audio.read())
    if st.button("ìŒì„± ë¶„ì„ ì‹œì‘"):
        transcript = safe_transcribe()
        result = analyze_with_ollama(f"Transcript:\n{transcript}\n\n{prompt_text}")
        st.subheader("ìŒì„± ë¶„ì„ ê²°ê³¼")
        st.code(transcript)
        st.write(result)

st.caption("Â© 2025 ì‹œì˜¨ë§ˆì¼€íŒ… | ê°œë°œì í™ì„í‘œ")
